{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "423a8ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "from statistics import mean, stdev\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96d9ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Config =========\n",
    "PROMPT = \"Olá, como você está?\"\n",
    "MODELS = [\"Gemma3:4b\", \"deepseek-r1:8b\"]  # None => usa todos instalados (ollama.list())\n",
    "N_RUNS = 3\n",
    "SAMPLE_INTERVAL_SEC = 0.0005\n",
    "\n",
    "# CSVs (opcional)\n",
    "OUT_CSV_SUMMARY_RAM = \"resumo_RAM.csv\"\n",
    "OUT_CSV_RUNS_RAM    = \"runs_RAM.csv\"\n",
    "OUT_CSV_SUMMARY_VRAM = \"resumo_VRAM.csv\"\n",
    "OUT_CSV_RUNS_VRAM    = \"runs_VRAM.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec4320d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVML OK (versão 12.576.88)\n"
     ]
    }
   ],
   "source": [
    "# ========= NVML / GPU =========\n",
    "NVML_OK = False\n",
    "NVML_MSG = \"NVML não inicializado\"\n",
    "try:\n",
    "    import pynvml\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        NVML_OK = True\n",
    "        NVML_MSG = f\"NVML OK (versão {pynvml.nvmlSystemGetNVMLVersion()})\"\n",
    "    except Exception as e:\n",
    "        NVML_MSG = f\"Falha nvmlInit: {e!r}\"\n",
    "except Exception as e:\n",
    "    NVML_MSG = f\"Falha import pynvml: {e!r}\"\n",
    "\n",
    "print(NVML_MSG)\n",
    "if not NVML_OK:\n",
    "    raise RuntimeError(\"NVML indisponível. Verifique driver NVIDIA e instalação do pynvml.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e54411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_ollama_pids() -> List[int]:\n",
    "    pids = []\n",
    "    for proc in psutil.process_iter(attrs=[\"pid\",\"name\"]):\n",
    "        try:\n",
    "            if \"ollama\" in (proc.info.get(\"name\") or \"\").lower():\n",
    "                pids.append(proc.info[\"pid\"])\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "            pass\n",
    "    return sorted(set(pids))\n",
    "\n",
    "def ram_mb_for_pids(pids: List[int]) -> float:\n",
    "    total = 0\n",
    "    for pid in pids:\n",
    "        try:\n",
    "            total += psutil.Process(pid).memory_info().rss\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "            pass\n",
    "    return total / (1024**2)\n",
    "\n",
    "def vram_used_mb_all_gpus() -> Dict[int, float]:\n",
    "    out: Dict[int, float] = {}\n",
    "    if not NVML_OK:\n",
    "        return out\n",
    "    try:\n",
    "        n = pynvml.nvmlDeviceGetCount()\n",
    "        for i in range(n):\n",
    "            h = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            mem = pynvml.nvmlDeviceGetMemoryInfo(h)\n",
    "            out[i] = mem.used / (1024**2)\n",
    "    except Exception:\n",
    "        return {}\n",
    "    return out\n",
    "\n",
    "def vram_peak_delta_mb_during(stream_iter, sample_interval_sec=0.05):\n",
    "    baseline = vram_used_mb_all_gpus()\n",
    "    peak_by_gpu = {k: 0.0 for k in baseline.keys()}\n",
    "    last_sample = time.perf_counter()\n",
    "\n",
    "    def sample():\n",
    "        current = vram_used_mb_all_gpus()\n",
    "        for gi in set(baseline.keys()) | set(current.keys()):\n",
    "            b = baseline.get(gi, 0.0)\n",
    "            c = current.get(gi, 0.0)\n",
    "            delta = max(0.0, c - b)\n",
    "            peak_by_gpu[gi] = max(peak_by_gpu.get(gi, 0.0), delta)\n",
    "\n",
    "    sample()  # <-- amostra inicial (novo)\n",
    "\n",
    "    for _ in stream_iter:\n",
    "        now = time.perf_counter()\n",
    "        if (now - last_sample) >= sample_interval_sec:\n",
    "            sample()\n",
    "            last_sample = now\n",
    "\n",
    "    sample()  # <-- amostra final (já tinha, mantenha)\n",
    "\n",
    "    peak_global = max(peak_by_gpu.values()) if peak_by_gpu else None\n",
    "    return peak_global, peak_by_gpu\n",
    "\n",
    "def get_models() -> List[str]:\n",
    "    if MODELS:\n",
    "        return MODELS\n",
    "    tags = ollama.list()\n",
    "    ms = [m[\"name\"] for m in tags.get(\"models\", []) if \"name\" in m]\n",
    "    if not ms:\n",
    "        raise RuntimeError(\"Nenhum modelo em `ollama list`.\")\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8d9c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Benchmark RAM =========\n",
    "def run_once_ram(model: str, prompt: str) -> dict:\n",
    "    pids = list_ollama_pids()\n",
    "    peak_ram_mb = 0.0\n",
    "    start = time.perf_counter()\n",
    "    last_sample = start\n",
    "\n",
    "    stream = ollama.generate(model=model, prompt=prompt, stream=True)\n",
    "    text_chunks = []\n",
    "    for chunk in stream:\n",
    "        if \"response\" in chunk:\n",
    "            text_chunks.append(chunk[\"response\"])\n",
    "        now = time.perf_counter()\n",
    "        if pids and (now - last_sample) >= SAMPLE_INTERVAL_SEC:\n",
    "            peak_ram_mb = max(peak_ram_mb, ram_mb_for_pids(pids))\n",
    "            last_sample = now\n",
    "\n",
    "    if pids:\n",
    "        peak_ram_mb = max(peak_ram_mb, ram_mb_for_pids(pids))\n",
    "\n",
    "    elapsed_wall = time.perf_counter() - start\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"elapsed_wall_sec\": elapsed_wall,\n",
    "        \"peak_ram_mb\": round(peak_ram_mb, 2) if peak_ram_mb else None,\n",
    "        \"output_preview\": (\"\".join(text_chunks))[:200]\n",
    "    }\n",
    "\n",
    "def best_of_n_ram(model: str, prompt: str, n: int):\n",
    "    runs = []\n",
    "    for i in range(1, n+1):\n",
    "        print(f\" - {model}: rodada {i}/{n}…\")\n",
    "        runs.append(run_once_ram(model, prompt))\n",
    "    ram_vals = [r[\"peak_ram_mb\"] for r in runs if r[\"peak_ram_mb\"] is not None]\n",
    "    t_vals = [r[\"elapsed_wall_sec\"] for r in runs if r[\"elapsed_wall_sec\"] is not None]\n",
    "    return runs, {\n",
    "        \"model\": model,\n",
    "        \"best_peak_ram_mb\": min(ram_vals) if ram_vals else None,\n",
    "        \"mean_peak_ram_mb\": mean(ram_vals) if ram_vals else None,\n",
    "        \"std_peak_ram_mb\": stdev(ram_vals) if len(ram_vals) > 1 else None,\n",
    "        \"best_elapsed_wall_sec\": min(t_vals) if t_vals else None,\n",
    "        \"mean_elapsed_wall_sec\": mean(t_vals) if t_vals else None,\n",
    "        \"std_elapsed_wall_sec\": stdev(t_vals) if len(t_vals) > 1 else None,\n",
    "    }\n",
    "\n",
    "# ========= Benchmark VRAM =========\n",
    "def run_once_vram(model: str, prompt: str) -> dict:\n",
    "    start = time.perf_counter()\n",
    "    stream = ollama.generate(model=model, prompt=prompt, stream=True)\n",
    "    text_preview = []\n",
    "\n",
    "    def _iter():\n",
    "        for chunk in stream:\n",
    "            if \"response\" in chunk:\n",
    "                text_preview.append(chunk[\"response\"])\n",
    "            yield chunk\n",
    "\n",
    "    peak_vram_delta_mb, peak_vram_delta_detail = vram_peak_delta_mb_during(_iter(), SAMPLE_INTERVAL_SEC)\n",
    "    elapsed_wall = time.perf_counter() - start\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"elapsed_wall_sec\": elapsed_wall,\n",
    "        \"peak_vram_mb\": round(peak_vram_delta_mb, 2) if peak_vram_delta_mb else None,\n",
    "        \"peak_vram_detail\": json.dumps({k: round(v,2) for k,v in peak_vram_delta_detail.items()}),\n",
    "        \"output_preview\": (\"\".join(text_preview))[:200]\n",
    "    }\n",
    "\n",
    "def best_of_n_vram(model: str, prompt: str, n: int):\n",
    "    runs = []\n",
    "    for i in range(1, n+1):\n",
    "        print(f\" - {model}: rodada {i}/{n}…\")\n",
    "        runs.append(run_once_vram(model, prompt))\n",
    "    vram_vals = [r[\"peak_vram_mb\"] for r in runs if r[\"peak_vram_mb\"] is not None]\n",
    "    t_vals = [r[\"elapsed_wall_sec\"] for r in runs if r[\"elapsed_wall_sec\"] is not None]\n",
    "    return runs, {\n",
    "        \"model\": model,\n",
    "        \"best_peak_vram_mb\": min(vram_vals) if vram_vals else None,\n",
    "        \"mean_peak_vram_mb\": mean(vram_vals) if vram_vals else None,\n",
    "        \"std_peak_vram_mb\": stdev(vram_vals) if len(vram_vals) > 1 else None,\n",
    "        \"best_elapsed_wall_sec\": min(t_vals) if t_vals else None,\n",
    "        \"mean_elapsed_wall_sec\": mean(t_vals) if t_vals else None,\n",
    "        \"std_elapsed_wall_sec\": stdev(t_vals) if len(t_vals) > 1 else None,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51ad7538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos: ['Gemma3:4b', 'deepseek-r1:8b']\n",
      "\n",
      "=== Gemma3:4b — RAM ===\n",
      " - Gemma3:4b: rodada 1/3…\n",
      " - Gemma3:4b: rodada 2/3…\n",
      " - Gemma3:4b: rodada 3/3…\n",
      "\n",
      "=== deepseek-r1:8b — RAM ===\n",
      " - deepseek-r1:8b: rodada 1/3…\n",
      " - deepseek-r1:8b: rodada 2/3…\n",
      " - deepseek-r1:8b: rodada 3/3…\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_peak_ram_mb</th>\n",
       "      <th>mean_peak_ram_mb</th>\n",
       "      <th>std_peak_ram_mb</th>\n",
       "      <th>best_elapsed_wall_sec</th>\n",
       "      <th>mean_elapsed_wall_sec</th>\n",
       "      <th>std_elapsed_wall_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemma3:4b</td>\n",
       "      <td>263.14</td>\n",
       "      <td>993.84</td>\n",
       "      <td>632.869473</td>\n",
       "      <td>0.284039</td>\n",
       "      <td>0.854702</td>\n",
       "      <td>0.916968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepseek-r1:8b</td>\n",
       "      <td>300.88</td>\n",
       "      <td>835.42</td>\n",
       "      <td>463.274259</td>\n",
       "      <td>3.409997</td>\n",
       "      <td>4.506795</td>\n",
       "      <td>1.185375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  best_peak_ram_mb  mean_peak_ram_mb  std_peak_ram_mb  \\\n",
       "0       Gemma3:4b            263.14            993.84       632.869473   \n",
       "1  deepseek-r1:8b            300.88            835.42       463.274259   \n",
       "\n",
       "   best_elapsed_wall_sec  mean_elapsed_wall_sec  std_elapsed_wall_sec  \n",
       "0               0.284039               0.854702              0.916968  \n",
       "1               3.409997               4.506795              1.185375  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>elapsed_wall_sec</th>\n",
       "      <th>peak_ram_mb</th>\n",
       "      <th>output_preview</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemma3:4b</td>\n",
       "      <td>1.912424</td>\n",
       "      <td>263.14</td>\n",
       "      <td>Olá! Eu estou bem, obrigado por perguntar! Com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gemma3:4b</td>\n",
       "      <td>0.367642</td>\n",
       "      <td>1368.24</td>\n",
       "      <td>Olá! Eu estou bem, obrigado por perguntar! Com...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemma3:4b</td>\n",
       "      <td>0.284039</td>\n",
       "      <td>1350.14</td>\n",
       "      <td>Olá! Eu estou bem, obrigado por perguntar! Com...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deepseek-r1:8b</td>\n",
       "      <td>5.764341</td>\n",
       "      <td>300.88</td>\n",
       "      <td>&lt;think&gt;\\nAh, o usuário começou com um “Olá, co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek-r1:8b</td>\n",
       "      <td>3.409997</td>\n",
       "      <td>1120.67</td>\n",
       "      <td>&lt;think&gt;\\nAh, o usuário começou com um cumprime...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek-r1:8b</td>\n",
       "      <td>4.346047</td>\n",
       "      <td>1084.71</td>\n",
       "      <td>&lt;think&gt;\\nAh, o usuário deu um oi simples em po...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  elapsed_wall_sec  peak_ram_mb  \\\n",
       "0       Gemma3:4b          1.912424       263.14   \n",
       "1       Gemma3:4b          0.367642      1368.24   \n",
       "2       Gemma3:4b          0.284039      1350.14   \n",
       "3  deepseek-r1:8b          5.764341       300.88   \n",
       "4  deepseek-r1:8b          3.409997      1120.67   \n",
       "5  deepseek-r1:8b          4.346047      1084.71   \n",
       "\n",
       "                                      output_preview  run  \n",
       "0  Olá! Eu estou bem, obrigado por perguntar! Com...    1  \n",
       "1  Olá! Eu estou bem, obrigado por perguntar! Com...    2  \n",
       "2  Olá! Eu estou bem, obrigado por perguntar! Com...    3  \n",
       "3  <think>\\nAh, o usuário começou com um “Olá, co...    1  \n",
       "4  <think>\\nAh, o usuário começou com um cumprime...    2  \n",
       "5  <think>\\nAh, o usuário deu um oi simples em po...    3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========= Execução RAM =========\n",
    "models = get_models()\n",
    "print(\"Modelos:\", models)\n",
    "\n",
    "all_runs_rows, summary_rows = [], []\n",
    "for m in models:\n",
    "    print(f\"\\n=== {m} — RAM ===\")\n",
    "    runs, summary = best_of_n_ram(m, PROMPT, N_RUNS)\n",
    "    for idx, r in enumerate(runs, 1):\n",
    "        all_runs_rows.append({**r, \"run\": idx})\n",
    "    summary_rows.append(summary)\n",
    "\n",
    "df_ram_runs = pd.DataFrame(all_runs_rows)\n",
    "df_ram_summary = pd.DataFrame(summary_rows)\n",
    "\n",
    "if OUT_CSV_RUNS_RAM: df_ram_runs.to_csv(OUT_CSV_RUNS_RAM, index=False)\n",
    "if OUT_CSV_SUMMARY_RAM: df_ram_summary.to_csv(OUT_CSV_SUMMARY_RAM, index=False)\n",
    "\n",
    "display(df_ram_summary, df_ram_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2490664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gemma3:4b — VRAM ===\n",
      " - Gemma3:4b: rodada 1/3…\n",
      " - Gemma3:4b: rodada 2/3…\n",
      " - Gemma3:4b: rodada 3/3…\n",
      "\n",
      "=== deepseek-r1:8b — VRAM ===\n",
      " - deepseek-r1:8b: rodada 1/3…\n",
      " - deepseek-r1:8b: rodada 2/3…\n",
      " - deepseek-r1:8b: rodada 3/3…\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_peak_vram_mb</th>\n",
       "      <th>mean_peak_vram_mb</th>\n",
       "      <th>std_peak_vram_mb</th>\n",
       "      <th>best_elapsed_wall_sec</th>\n",
       "      <th>mean_elapsed_wall_sec</th>\n",
       "      <th>std_elapsed_wall_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemma3:4b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.227735</td>\n",
       "      <td>0.809710</td>\n",
       "      <td>0.961347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepseek-r1:8b</td>\n",
       "      <td>1.06</td>\n",
       "      <td>300.273333</td>\n",
       "      <td>516.24482</td>\n",
       "      <td>2.000494</td>\n",
       "      <td>3.526182</td>\n",
       "      <td>1.588496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  best_peak_vram_mb  mean_peak_vram_mb  std_peak_vram_mb  \\\n",
       "0       Gemma3:4b                NaN                NaN               NaN   \n",
       "1  deepseek-r1:8b               1.06         300.273333         516.24482   \n",
       "\n",
       "   best_elapsed_wall_sec  mean_elapsed_wall_sec  std_elapsed_wall_sec  \n",
       "0               0.227735               0.809710              0.961347  \n",
       "1               2.000494               3.526182              1.588496  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>elapsed_wall_sec</th>\n",
       "      <th>peak_vram_mb</th>\n",
       "      <th>peak_vram_detail</th>\n",
       "      <th>output_preview</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemma3:4b</td>\n",
       "      <td>1.919334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"0\": 0.0}</td>\n",
       "      <td>Olá! Eu estou bem, obrigado por perguntar! Com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gemma3:4b</td>\n",
       "      <td>0.227735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"0\": 0.0}</td>\n",
       "      <td>Olá! Eu estou bem, obrigado por perguntar! Com...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemma3:4b</td>\n",
       "      <td>0.282060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"0\": 0.0}</td>\n",
       "      <td>Olá! Eu estou bem, obrigado por perguntar! Com...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deepseek-r1:8b</td>\n",
       "      <td>5.170800</td>\n",
       "      <td>896.38</td>\n",
       "      <td>{\"0\": 896.38}</td>\n",
       "      <td>&lt;think&gt;\\nHmm…… o usuário começou com um cumpri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek-r1:8b</td>\n",
       "      <td>2.000494</td>\n",
       "      <td>1.06</td>\n",
       "      <td>{\"0\": 1.06}</td>\n",
       "      <td>&lt;think&gt;\\nAh, o usuário começou com um “Olá” su...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek-r1:8b</td>\n",
       "      <td>3.407253</td>\n",
       "      <td>3.38</td>\n",
       "      <td>{\"0\": 3.38}</td>\n",
       "      <td>&lt;think&gt;\\nAh, o usuário começou com um cumprime...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  elapsed_wall_sec  peak_vram_mb peak_vram_detail  \\\n",
       "0       Gemma3:4b          1.919334           NaN       {\"0\": 0.0}   \n",
       "1       Gemma3:4b          0.227735           NaN       {\"0\": 0.0}   \n",
       "2       Gemma3:4b          0.282060           NaN       {\"0\": 0.0}   \n",
       "3  deepseek-r1:8b          5.170800        896.38    {\"0\": 896.38}   \n",
       "4  deepseek-r1:8b          2.000494          1.06      {\"0\": 1.06}   \n",
       "5  deepseek-r1:8b          3.407253          3.38      {\"0\": 3.38}   \n",
       "\n",
       "                                      output_preview  run  \n",
       "0  Olá! Eu estou bem, obrigado por perguntar! Com...    1  \n",
       "1  Olá! Eu estou bem, obrigado por perguntar! Com...    2  \n",
       "2  Olá! Eu estou bem, obrigado por perguntar! Com...    3  \n",
       "3  <think>\\nHmm…… o usuário começou com um cumpri...    1  \n",
       "4  <think>\\nAh, o usuário começou com um “Olá” su...    2  \n",
       "5  <think>\\nAh, o usuário começou com um cumprime...    3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========= Execução VRAM =========\n",
    "all_runs_rows, summary_rows = [], []\n",
    "for m in models:\n",
    "    print(f\"\\n=== {m} — VRAM ===\")\n",
    "    runs, summary = best_of_n_vram(m, PROMPT, N_RUNS)\n",
    "    for idx, r in enumerate(runs, 1):\n",
    "        all_runs_rows.append({**r, \"run\": idx})\n",
    "    summary_rows.append(summary)\n",
    "\n",
    "df_vram_runs = pd.DataFrame(all_runs_rows)\n",
    "df_vram_summary = pd.DataFrame(summary_rows)\n",
    "\n",
    "if OUT_CSV_RUNS_VRAM: df_vram_runs.to_csv(OUT_CSV_RUNS_VRAM, index=False)\n",
    "if OUT_CSV_SUMMARY_VRAM: df_vram_summary.to_csv(OUT_CSV_SUMMARY_VRAM, index=False)\n",
    "\n",
    "display(df_vram_summary, df_vram_runs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
